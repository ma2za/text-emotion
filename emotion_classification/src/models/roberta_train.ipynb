{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59LLJSo6wHaQ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ma2za/emotion-classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6g7WexjxIyz"
      },
      "outputs": [],
      "source": [
        "!mv emotion-classification/emotion_classification/src/roberta_emotion roberta_emotion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTvyV2vhFjJ3"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate wandb \"ray[tune]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTZp94-9A8Mf"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import random\n",
        "from functools import partial\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "from datasets import ClassLabel, Features, Value\n",
        "from datasets import Dataset\n",
        "from datasets import concatenate_datasets\n",
        "from datasets import load_dataset\n",
        "from evaluate import evaluator\n",
        "from huggingface_hub import notebook_login\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers.data.data_collator import default_data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhInzzztxy9k"
      },
      "outputs": [],
      "source": [
        "from roberta_emotion.modeling_roberta_emotion import RobertaEmotion\n",
        "from roberta_emotion.configuration_roberta_emotion import RobertaEmotionConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64i4NV5z7hsl"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egzJy5JvESV0"
      },
      "outputs": [],
      "source": [
        "%env RAY_PICKLE_VERBOSE_DEBUG=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvvXTHMPrGjt"
      },
      "outputs": [],
      "source": [
        "%env WANDB_PROJECT=emotion_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnIPHQ7-qsnQ"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUeYzOHrq26P"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq8FexcaGMGw"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIHT0h-uvy_i"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoUKEHGTajux"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtFKa4SVageE"
      },
      "outputs": [],
      "source": [
        "def tokenization(sample):\n",
        "    return tokenizer(sample[\"text\"], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZaZE0YPvw0Y"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLOQE-yrGjhZ"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"emotion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS9987hIjrYY"
      },
      "outputs": [],
      "source": [
        "daily_dialog = load_dataset(\"daily_dialog\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0GxQFWlj-sK"
      },
      "outputs": [],
      "source": [
        "daily_dialog = concatenate_datasets([daily_dialog[\"train\"], daily_dialog[\"validation\"], daily_dialog[\"test\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzGFn9-onHeE"
      },
      "outputs": [],
      "source": [
        "def merge_daily_dialog(examples):\n",
        "    dd2emo = {\n",
        "        1: \"anger\",\n",
        "        3: \"fear\",\n",
        "        4: \"joy\",\n",
        "        5: \"sadness\",\n",
        "        6: \"surprise\"\n",
        "    }\n",
        "\n",
        "    return {\"chunks\": [ {\"text\": d, \"label\": dd2emo[e]} for d, e in zip(examples[\"dialog\"], examples[\"emotion\"]) if e in [1, 3, 4, 5, 6] and len(d) < 85]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YijMYm70lCp6"
      },
      "outputs": [],
      "source": [
        "temp = daily_dialog.map(merge_daily_dialog, remove_columns=daily_dialog.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOHSWMVnxw40"
      },
      "outputs": [],
      "source": [
        "features = Features({'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None) ,\n",
        " 'text': Value(dtype='string', id=None)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdD70sT6rBkN"
      },
      "outputs": [],
      "source": [
        "daily_dialog = Dataset.from_pandas(pd.DataFrame(list(itertools.chain(*temp[\"chunks\"]))),\n",
        "                                   features=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u_SQVbKujXU"
      },
      "outputs": [],
      "source": [
        "DATASETS = [\"emotion\"]\n",
        "#DATASETS = [\"emotion\", \"daily_dialog\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLnniNrpum05"
      },
      "outputs": [],
      "source": [
        "if len(DATASETS) != 1:\n",
        "    dataset[\"train\"]  = concatenate_datasets([dataset[\"train\"], daily_dialog])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Y-r15obTPp"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(tokenization, batched=True, batch_size=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGsjKIq_wNWa"
      },
      "outputs": [],
      "source": [
        "dataset.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrpWsH2-rFk0"
      },
      "outputs": [],
      "source": [
        "id2label =  {\n",
        "    0: \"sadness\",\n",
        "    1: \"joy\",\n",
        "    2: \"love\",\n",
        "    3: \"anger\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "  }\n",
        "\n",
        "label2id = {\n",
        "    \"sadness\": 0,\n",
        "    \"joy\": 1,\n",
        "    \"love\": 2,\n",
        "    \"anger\": 3,\n",
        "    \"fear\": 4,\n",
        "    \"surprise\": 5\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDqhkAzgPDIX"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "train_dataset.remove_columns([\"text\"])\n",
        "\n",
        "valid_dataset = dataset[\"validation\"]\n",
        "valid_dataset.remove_columns([\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbw8tpwfwDyE"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v-Yu1fXZ2kb"
      },
      "outputs": [],
      "source": [
        "RobertaEmotionConfig.register_for_auto_class()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETurkMwqZ8Ry"
      },
      "outputs": [],
      "source": [
        "RobertaEmotion.register_for_auto_class(\"AutoModel\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = {\n",
        "    \"id2label\": id2label,\n",
        "    \"label2id\": label2id,\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_labels\": 6\n",
        "}"
      ],
      "metadata": {
        "id": "MtuwkH9wWe7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY2KC7O3j938"
      },
      "outputs": [],
      "source": [
        "emotion_config = RobertaEmotionConfig(**model_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGXulHvDwQq0"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y8b4S-enJ3v"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(preds, labels):\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHEM40l95uqf"
      },
      "outputs": [],
      "source": [
        "def evaluation(model, dataloader):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_f1 = 0\n",
        "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        acc, f1 = compute_metrics(outputs.logits.argmax(-1).detach().cpu(), labels.detach().cpu())\n",
        "        total_acc += acc*len(labels)\n",
        "        total_f1 += f1*len(labels)\n",
        "        total_samples += len(labels)\n",
        "        total_loss += outputs.loss.detach().cpu()*len(labels)\n",
        "    return total_acc/total_samples, total_f1/total_samples, total_loss/total_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUNPC0AJRMLa"
      },
      "outputs": [],
      "source": [
        "def train(model, checkpoint_dir, optimizer, lr_scheduler, train_loader, \n",
        "          valid_loader, tune_flag=False, config={}):\n",
        "    wandb.init(project=\"emotion_classifier\", config=config)\n",
        "\n",
        "    best_f1 = 0\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        model.train()\n",
        "        for step, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            model.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            outputs.loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "        valid_acc, valid_f1, valid_loss = evaluation(model, valid_loader)\n",
        "        wandb.log({\"eval/loss\": valid_loss, \"eval/f1\": valid_f1, \n",
        "                   \"eval/accuracy\": valid_acc, \n",
        "                   \"train/loss\": outputs.loss.detach().cpu()})\n",
        "\n",
        "        if tune_flag:\n",
        "\n",
        "            with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "                path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "                torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "            tune.report(loss=valid_loss, accuracy=valid_acc)\n",
        "        else:\n",
        "            if best_f1 < valid_f1:\n",
        "                best_f1 = valid_f1\n",
        "                path = os.path.join(checkpoint_dir, \"pytorch_model.bin\")\n",
        "                torch.save(model.state_dict(), path)\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGuuTj5N-59u"
      },
      "outputs": [],
      "source": [
        "def train_roberta(config: Dict, checkpoint_dir: str = None):\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        collate_fn=default_data_collator,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        collate_fn=default_data_collator,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    num_warmup_steps = float(len(train_loader) * config[\"epochs\"] * config[\"warmup_ratio\"])\n",
        "\n",
        "    model = RobertaEmotion(emotion_config).to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=config[\"lr\"], betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "    def lr_lambda(x: float, warmup: float):\n",
        "        return 1.0 if x >= warmup else x / warmup\n",
        "\n",
        "    lr_scheduler = LambdaLR(optimizer, partial(lr_lambda, warmup=num_warmup_steps))\n",
        "    train(model, checkpoint_dir, optimizer, lr_scheduler, train_loader, valid_loader, config=config)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-RZW8IRHJr9"
      },
      "outputs": [],
      "source": [
        "def tuning():\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    config = {\n",
        "        \"batch_size\": tune.choice([32, 64, 128])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=10,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # ``parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"]``,\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_roberta, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 1, \"gpu\": 1},\n",
        "        config=config,\n",
        "        num_samples=10,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "\n",
        "    best_trained_model = RobertaEmotion(emotion_config).to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzn9djetuLmr"
      },
      "outputs": [],
      "source": [
        "train_config = {\"batch_size\": 64,\n",
        "                \"epochs\": 25,\n",
        "                \"lr\": 1e-05,\n",
        "                \"warmup_ratio\": 0.2,\n",
        "                \"datasets\": DATASETS}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndJjf5uXHMdD"
      },
      "outputs": [],
      "source": [
        "model = train_roberta(train_config, checkpoint_dir=\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NOMeUZSDPcE"
      },
      "outputs": [],
      "source": [
        "model_state = torch.load(os.path.join(\".\", \"pytorch_model.bin\"))\n",
        "model.load_state_dict(model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBZ9vsTzC51f"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"roberta-emotion\")\n",
        "tokenizer.push_to_hub(\"roberta-emotion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkzTXnTWwVbT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG4Csiq44C_u"
      },
      "outputs": [],
      "source": [
        "task_evaluator = evaluator(\"text-classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4sBChKK6wnp"
      },
      "outputs": [],
      "source": [
        "results = task_evaluator.compute(\n",
        "    model_or_pipeline=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data=\"emotion\",\n",
        "    subset=\"split\",\n",
        "    split=\"test\",\n",
        "    metric=\"accuracy\",\n",
        "    label_mapping=label2id,\n",
        "    strategy=\"bootstrap\",\n",
        "    n_resamples=10,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO_00j6hoA7J"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}